\chapter{Implementation}
In this chapter we describe in details about the framework that we have designed and implemented for testing the SQL-compliance for current DBMSs. The framework is composed by different tools such random SQL generator engine, SQL results comparison tool and random data generator. 

\section{Random SQL generator engine}
An important component of the framework is the random SQL generator tool which can be used to generate thousand of different queries for evaluating the current DBMSs. It is worthy mentioning that the tool can be used by its own use as well. For that reason, we have designed and implemented this tool from scratch using Java programming language. In addition, the tool has been designed in a way that is modular and reusable for supporting easily new DBMSs in the future by adding a small portion of code. More precisely, the main idea of the tool is to generate a diversity of queries for testing the DBMSs.   

\subsubsection{Implementation details}
The random SQL generator tool is consisted by different Java classes and a configuration file. The most important decision that we had to take was how we will design our engine in a way that will make it possible to generate a diversity of different valid SQL queries. Hence, we have an internal representation for each SQL query and each class is responsible for generating a different statement  that contribute to the overall query. For example, one of the classes that constitutes the tool is the SELECT class. Apparently, this class is responsible for generating the SELECT clause of each SQL query. Having different classes for each clause, it makes it easier to extend the tool in order to add new functionality and at the same time there is no need to change other part of the code. The final SQL query is converted to an SQL string and is executed to the current DBMSs. Obviously, we could not generate SQL strings directly because we need to track things. If you generating strings, it will not possible to check if in the WHERE clause you mentioning attributes that exist in the FROM clause or they comes as parameters from the outer query. Thus, there is a need to track attributes for each clause. In that way we have achieving implement a tool with high cohesion.
Another important problem that we have to handle was how to avoid generating completely random SQL queries. Even though we need to generate random SQL queries to stress the current DBMSs in different situations, we need to control some characteristics of the SQL query. For example, imagine an SQL query that performs a cartesian product with a large number of tables. In this situation, we will have as a result the query to be executed for a long time or even for ever and it is not so useful for our case. Parameters such as maximum level of nesting and max number of tables in a FROM clause can be given in the configuration file. As a consequence, our tool supports a configuration file that can be used to control the randomly generated SQL queries. Below is provided a part of the configuration file and some parameters are explained.   


It can be seen from the configuration file that we can control many parameters. For example, we can set an upper bound of tables that we can have in the FROM clause. In this way, we avoid having an enormous table from cartesian product. In addition, even it is not so usual to have constant comparison in an SQL query, nevertheless SQL standards support this. Thus, we generate SQL queries which have constant comparisons but we do not need to have a lot of such queries. As a result, we add a parameter for determine the probability of having such cases. The parameters name is probWhrConst which have a domain between 0 and 1 meaning that if the value is 1 that there will be exist for sure a constant comparison.
Another important decision that should be taken into account is how we can provide the relations and attributes to the tool. An initial approach was to be given as parameters in the configuration file. Albeit this approach works pretty well, it makes our tool not portable. Image if the DBMSs have lots of tables with many columns. Hence, It will be time-consuming to give these parameters via the configuration files. Thus, an efficient approach is to retrieve the whole schema from DBMSs automatically. As a result, our tool has the capability to automatically retrieve the whole schema for any DBMS just by providing the credential for connecting to the database in the configuration file.    


\section{Comparison Tool} 


\section{Generate data}
Datafiller is a well-known open-source project that provides the capability of generating random data.Thus we will use this project for generating a data set in order to evaluate it in all DBMS. More precisely, the datafiller script generates random data, based on a data schema which is given as a parameter, and by taking into account constraints of that schema. For example, it takes into account the domain of each field and if the field should be unique, foreign key or primary key. Another important parameter is the df: null=x.xx% which indicated nullable rates. It will be extremely useful to test the behaviour of current DBMS in a database with nulls and check if there are differences.     
 
Additionally, more complex parameters can be provided as well, such as a number of tuples per table using --size SIZE parameter. It worthy mentioning that these parameters should be defined within the schema script and should start with -- df.  Further, we can generate more realistic data by providing some information in the schema script of the database. For example, if we have a field that represents a date, then we can provide a specific range in order for the datafiller to generate dates only within this range. This can be achieved by specifying the following parameter: range -- df: start=year-month-day end=year-month-day beside the date field. Subsequently, we need to add the --filter parameter while running the script. These are only some of the important parameters that the datafiller provides but apart from these, it provides more sophisticated properties which are out of importance for our project.
 
It is important mentioning that the datafiller does not support importing data to other databases such as Oracle database, MS Server or IBM DB2 except postgresSQL. As a result, our approach is to import the data in postgreSQL and then export the random generated data in a CSV files. In this way, we can import the CSV files in all the DBMS, as all of them support importing data from CSV files.